{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的tokenizer\n",
    "\n",
    "英文都会拆到char\n",
    "\n",
    "基本相当于list(inputs_str.replace(' ', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "word_index = {}\n",
    "special = []\n",
    "with open('./vocab.txt') as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        line = line.strip().lower()\n",
    "        word_index[line] = i\n",
    "        if line.startswith('[') and line.endswith(']'):\n",
    "            special.append(line)\n",
    "\n",
    "print(len(special))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[pad]', '[unused1]', '[unused2]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(tf.keras.models.Model):\n",
    "    def __init__(self, word_index, **args):\n",
    "        super(BertTokenizer, self).__init__(**args)\n",
    "        self.construct(word_index)\n",
    "    \n",
    "    def construct(self, word_index):\n",
    "        keys = tf.constant(list(word_index.keys()), dtype=tf.string)\n",
    "        values = tf.constant(list(word_index.values()), dtype=tf.int32)\n",
    "        self.table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(keys, values),\n",
    "            tf.constant(word_index['[unk]'])) # default value\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # x = tf.strings.unicode_encode(x, 'UTF-8', errors='ignore')\n",
    "        x = tf.strings.lower(x)\n",
    "        x = tf.strings.regex_replace(x, tf.constant(r'\\s'), tf.constant('-'))\n",
    "        x = tf.strings.unicode_split(x, 'UTF-8')\n",
    "        x = tf.strings.reduce_join(x, separator=' ', axis=-1)\n",
    "        x = tf.strings.regex_replace(x, r'\\[ p a d \\]', ' [pad] ')\n",
    "        x = tf.strings.regex_replace(x, r'\\[ u n k \\]', ' [unk] ')\n",
    "        x = tf.strings.regex_replace(x, r'\\[ c l s \\]', ' [cls] ')\n",
    "        x = tf.strings.regex_replace(x, r'\\[ s e p \\]', ' [sep] ')\n",
    "        x = tf.strings.regex_replace(x, r'\\[ m a s k \\]', ' [mask] ')\n",
    "        x = tf.strings.split(x)\n",
    "        x = x.to_tensor('[pad]')\n",
    "        x = self.table.lookup(x)\n",
    "        x = tf.squeeze(x, 1)\n",
    "\n",
    "        cls = tf.fill([tf.shape(x)[0], 1], tf.constant(101))\n",
    "        pad = tf.fill([tf.shape(x)[0], 1], tf.constant(0))\n",
    "        x = x[:, :510]\n",
    "        x = tf.concat([cls, x, pad], axis=1)\n",
    "    \n",
    "        row_inds = tf.range(0, tf.shape(x)[0])\n",
    "        col_inds = tf.math.count_nonzero(x, axis=1)\n",
    "        col_inds = tf.cast(col_inds, tf.int32)\n",
    "        inds = tf.concat([\n",
    "            tf.reshape(row_inds, (-1, 1)),\n",
    "            tf.reshape(col_inds, (-1, 1))], axis=1)\n",
    "        fill = tf.ones(tf.shape(x)[0], dtype=tf.int32)\n",
    "        shape = tf.cast(tf.shape(x), tf.int32)\n",
    "        sep = tf.scatter_nd(inds, fill, shape) * tf.constant(102)\n",
    "        x = x + sep\n",
    "    \n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = BertTokenizer(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
       "array([[101, 143, 143, 102],\n",
       "       [101, 143, 102,   0]], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t([['aa'], ['a']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([['aa'], ['a']] * 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 ms ± 41.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "save_path = 'bert_tokenizer'\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     BertTokenizer(word_index, name='bert_tokenizer'),\n",
    "#     BertIds(word_index, name='bert_token_to_ids'),\n",
    "# ])\n",
    "# model.save(save_path, include_optimizer=False)\n",
    "model = BertTokenizer(word_index)\n",
    "# model._set_inputs(tf.keras.backend.placeholder((None, None), dtype=tf.int32))\n",
    "model._set_inputs(tf.keras.backend.placeholder((None, 1), dtype='string'))\n",
    "model.save(save_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_tokenizer_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "m2 = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    ['我爱你[mask] [mask]哦'],\n",
    "    ['我[unk][MASK]'],\n",
    "    ['important']\n",
    "]\n",
    "vec = tf.constant(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 11), dtype=int32, numpy=\n",
       "array([[ 101, 2769, 4263,  872,  103,  118,  103, 1521,  102,    0,    0],\n",
       "       [ 101, 2769,  100,  103,  102,    0,    0,    0,    0,    0,    0],\n",
       "       [ 101,  151,  155,  158,  157,  160,  162,  143,  156,  162,  102]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
